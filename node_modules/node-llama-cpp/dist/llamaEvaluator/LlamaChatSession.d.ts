/// <reference types="node" />
import { ChatPromptWrapper } from "../ChatPromptWrapper.js";
import { ConversationInteraction, Token } from "../types.js";
import { LlamaContext } from "./LlamaContext.js";
import { LlamaGrammar } from "./LlamaGrammar.js";
export type LlamaChatSessionOptions = {
    context: LlamaContext;
    printLLamaSystemInfo?: boolean;
    /** GeneralChatPromptWrapper is ued by default */
    promptWrapper?: ChatPromptWrapper | "auto";
    systemPrompt?: string;
    /** Conversation history to load into the context to continue an existing conversation */
    conversationHistory?: readonly ConversationInteraction[];
};
export type LLamaChatPromptOptions = {
    onToken?: (tokens: Token[]) => void;
    signal?: AbortSignal;
    maxTokens?: number;
    /**
     * Temperature is a hyperparameter that controls the randomness of the generated text.
     * It affects the probability distribution of the model's output tokens.
     * A higher temperature (e.g., 1.5) makes the output more random and creative,
     * while a lower temperature (e.g., 0.5) makes the output more focused, deterministic, and conservative.
     * The suggested temperature is 0.8, which provides a balance between randomness and determinism.
     * At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run.
     *
     * Set to `0` to disable.
     * Disabled by default (set to `0`).
     */
    temperature?: number;
    /**
     * Limits the model to consider only the K most likely next tokens for sampling at each step of sequence generation.
     * An integer number between `1` and the size of the vocabulary.
     * Set to `0` to disable (which uses the full vocabulary).
     *
     * Only relevant when `temperature` is set to a value greater than 0.
     */
    topK?: number;
    /**
     * Dynamically selects the smallest set of tokens whose cumulative probability exceeds the threshold P,
     * and samples the next token only from this set.
     * A float number between `0` and `1`.
     * Set to `1` to disable.
     *
     * Only relevant when `temperature` is set to a value greater than `0`.
     */
    topP?: number;
    grammar?: LlamaGrammar;
    /**
     * Trim whitespace from the end of the generated text
     * Disabled by default.
     */
    trimWhitespaceSuffix?: boolean;
    repeatPenalty?: false | LlamaChatSessionRepeatPenalty;
};
export type LlamaChatSessionRepeatPenalty = {
    /**
     * Number of recent tokens generated by the model to apply penalties to repetition of.
     * Defaults to `64`.
     */
    lastTokens?: number;
    punishTokensFilter?: (tokens: Token[]) => Token[];
    /**
     * Penalize new line tokens.
     * Enabled by default.
     */
    penalizeNewLine?: boolean;
    /**
     * The relative amount to lower the probability of the tokens in `punishTokens` by
     * Defaults to `1.1`.
     * Set to `1` to disable.
     */
    penalty?: number;
    /**
     * For n time a token is in the `punishTokens` array, lower its probability by `n * frequencyPenalty`
     * Disabled by default (`0`).
     * Set to a value between `0` and `1` to enable.
     */
    frequencyPenalty?: number;
    /**
     * Lower the probability of all the tokens in the `punishTokens` array by `presencePenalty`
     * Disabled by default (`0`).
     * Set to a value between `0` and `1` to enable.
     */
    presencePenalty?: number;
};
export declare class LlamaChatSession {
    private readonly _systemPrompt;
    private readonly _printLLamaSystemInfo;
    private readonly _promptWrapper;
    private _promptIndex;
    private _initialized;
    private _lastStopString;
    private _lastStopStringSuffix;
    private _conversationHistoryToLoad;
    private readonly _ctx;
    /**
     * @param {LlamaChatSessionOptions} options
     */
    constructor({ context, printLLamaSystemInfo, promptWrapper, systemPrompt, conversationHistory }: LlamaChatSessionOptions);
    get initialized(): boolean;
    get context(): LlamaContext;
    init(): Promise<void>;
    /**
     * @param {string} prompt
     * @param {object} options
     * @returns {Promise<string>}
     */
    prompt(prompt: string, { onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix, repeatPenalty }?: LLamaChatPromptOptions): Promise<string>;
    /**
     * @param {string} prompt
     * @param {LLamaChatPromptOptions} options
     */
    promptWithMeta(prompt: string, { onToken, signal, maxTokens, temperature, topK, topP, grammar, trimWhitespaceSuffix, repeatPenalty }?: LLamaChatPromptOptions): Promise<{
        text: string;
        stopReason: "maxTokens" | "eosToken" | "stopString";
        stopString: string | null;
        stopStringSuffix: string | null;
    }>;
    private _evalTokens;
    private _checkStopString;
}

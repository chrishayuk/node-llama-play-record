import { Token } from "../types.js";
import { LlamaModel } from "./LlamaModel.js";
import { LlamaGrammarEvaluationState } from "./LlamaGrammarEvaluationState.js";
import { LlamaGrammar } from "./LlamaGrammar.js";
export type LlamaContextOptions = {
    model: LlamaModel;
    prependBos?: boolean;
    /**
     * @deprecated use the `grammar` option on `LlamaChatSession`'s `prompt` function
     * or the `grammarEvaluationState` option on `LlamaContext`'s `evaluate` function instead
     * @hidden
     */
    grammar?: LlamaGrammar;
    /** If null, a random seed will be used */
    seed?: number | null;
    /** text context size */
    contextSize?: number;
    /** prompt processing batch size */
    batchSize?: number;
    /** use fp16 for KV cache */
    f16Kv?: boolean;
    /** the llama_eval() call computes all logits, not just the last one */
    logitsAll?: boolean;
    /** embedding mode only */
    embedding?: boolean;
    /** number of threads to use to evaluate tokens */
    threads?: number;
};
export type LlamaContextRepeatPenalty = {
    /** Tokens to lower the predication probability of to be the next predicted token */
    punishTokens: Uint32Array | (() => Uint32Array);
    /**
     * The relative amount to lower the probability of the tokens in `punishTokens` by
     * Defaults to `1.1`.
     * Set to `1` to disable.
     */
    penalty?: number;
    /**
     * For n time a token is in the `punishTokens` array, lower its probability by `n * frequencyPenalty`
     * Disabled by default (`0`).
     * Set to a value between `0` and `1` to enable.
     */
    frequencyPenalty?: number;
    /**
     * Lower the probability of all the tokens in the `punishTokens` array by `presencePenalty`
     * Disabled by default (`0`).
     * Set to a value between `0` and `1` to enable.
     */
    presencePenalty?: number;
};
export declare class LlamaContext {
    private readonly _model;
    private readonly _ctx;
    private readonly _prependBos;
    private _prependTokens;
    /**
     * @param {LlamaContextOptions} options
     */
    constructor({ model, prependBos, grammar, seed, contextSize, batchSize, f16Kv, logitsAll, embedding, threads }: LlamaContextOptions);
    encode(text: string): Uint32Array;
    decode(tokens: Uint32Array | Token[]): string;
    get prependBos(): boolean;
    /**
     * @returns {Token | null} The BOS (Beginning Of Sequence) token.
     */
    getBosToken(): Token | null;
    /**
     * @returns {Token | null} The EOS (End Of Sequence) token.
     */
    getEosToken(): Token | null;
    /**
     * @returns {Token | null} The NL (New Line) token.
     */
    getNlToken(): Token | null;
    /**
     * @returns {string | null} The BOS (Beginning Of Sequence) token as a string.
     */
    getBosString(): string | null;
    /**
     * @returns {string | null} The EOS (End Of Sequence) token as a string.
     */
    getEosString(): string | null;
    /**
     * @returns {string | null} The NL (New Line) token as a string.
     */
    getNlString(): string | null;
    getContextSize(): number;
    /**
     * @param {Uint32Array} tokens
     * @param {object} options
     * @returns {AsyncGenerator<Token, void>}
     */
    evaluate(tokens: Uint32Array, { temperature, topK, topP, grammarEvaluationState, repeatPenalty }?: {
        temperature?: number;
        topK?: number;
        topP?: number;
        grammarEvaluationState?: LlamaGrammarEvaluationState;
        repeatPenalty?: LlamaContextRepeatPenalty;
    }): AsyncGenerator<Token, void>;
}
